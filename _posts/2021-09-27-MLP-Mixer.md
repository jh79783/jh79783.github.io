---
layout: post
title: MLP-Mixer
data: 2021-09-27
excerpt: "mlp"
tags: [mlp]
coments: false
mathjax: true
---

# MLP-Mixer

- 제목: MLP-Mixer: An all-MLP Architecture for Vision
- 저자: llya Tolstikhin, Neil Houlsby 외 10명 Google Research, Brain Team
- 인용수: 60회
- 학술지: arXiv 2021



## Introduction

- computer vision에서 CNN은 거의 표준이 되어있는 상태이지만, 최근 SOTA를 달성한 모델은 transformer(self attention)를 사용한 모델이 주를 이루고 있음

- transformer만을 사용하여 SOTA를 달성하였으니 본 논문에서는 convolution이나 transformer(self attention)없이 기술적으로 간단한 MLP만을 사용한 모델을 제안하며 이를 MLP-Mixer라고 지칭하였음

- MLP-Mixer layer는 두 타입이 존재

  1. channel-mixing MLP
  2. token-mixing MLP

- channel-mixing MLP는 pointwise convolution(1\*1 convolution)으로 볼 수 있고, token-mixing MLP는 depth-wise convolution과 비슷한 역할을 한다고 볼 수 있음

  > depth-wise convolution: 채널별로 따로 convolution을 진행하는 것
  >
  > [참고자료](https://eehoeskrap.tistory.com/431)

- ![](C:\workspace\논문\mlp-mixer\channel mixing.png)

- ![](C:\workspace\논문\mlp-mixer\token mixing.png)

- 두 타입의 layer를 사용하여 모델을 구성하였음

## Mixer Architecture

![](C:\workspace\논문\mlp-mixer\fig1.png)

- Mixer는 VIT와 비슷하게 image patch를 입력받는데, 이때 패치 수를 S, hidden dimension를 C라하는 데이터를 갖는 2차원 데이터를 input으로 함
- 이미지가 H, W의 resolution을 갖고있다면, patch는 P, P의 resolution을 갖게되고, patch수 $S=HW/P^2$가 됨
- Mixer layer는 두 MLP block으로 구성되어 있음
- Mixer layer에 받은 input을 transpose해주어 chanel과 patch의 위치를 바꾸어 준 후, token mixing을 진행
- 그 후 다시 transpose해주어 channel mixing을 진행
- MLP는 2개의 fully-connected layer와 activation으로 GELU를 사용한 형태
- Mixer layer에는 tunable hidden width가 존재함
- 이 tunable hidden width는 VIT와는 다르게 input patch(sequnce lenght)의 수에 영향을 받지 않기 때문에 computational complexity가 linear하게 증가할 수 있음 
- MLP가 input token의 순서에 sensitive하기 때문에 Mixer내부에서는 positional embbeding을 사용하지 않음

> VIT와 MLP Mixer는 사실 convolution을 사용했다!
>
> per-patch FC부분을 보면 사실 16*16convolution에 stride를 16을 준것과 같으며(VIT도 같은 방식), channel-mixing MLP는 그냥 1\*1 convolution이다!(Yann LeCun)
>
> ![](C:\workspace\논문\mlp-mixer\mixer code.png)

## Experiment

- VIT와 비슷하게 pre training을 사용하였음
  - ImageNet, ImageNet-21k, JFT-300M을 사용
- pre train:
  - Adam: $\beta_1=0.9,\beta_2=0.999$
  - batch size = 4096
- fine tuning:
  - SGD
  - batch size = 512

![](C:\workspace\논문\mlp-mixer\table1.png)

![](C:\workspace\논문\mlp-mixer\table2.png)

> pink: MPL-Mixer
>
> blue: VIT
>
> yellow: CNN

![](C:\workspace\논문\mlp-mixer\table3.png)

