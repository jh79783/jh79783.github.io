---
layout: post
title: YOLO v1
data: 2021-03-08
excerpt: "YOLO v1"
tags: [yolo, detector]
coments: false
mathjax: true
---

# YOLO v1

YOLO v1이라 불리는 논문의 제목은 *You Only Look Once: Unified, Real-Time Object Detection*입니다.

CVPR16에서 공개가 되었으며 21년3월 기준 약 1.4만회가 인용되었습니다.

저자로는 Joseph Redmon외 3명인데 R-CNN의 저자인 Ross Girshick도 저자로 참여하였습니다.

## Abstract

YOLO는 하나의 neural network가 전체 이미지에서 bbox와 클래스 확률을 예측할 수 있다고 합니다. pipline이 single network이기 때문에 detection 성능이 end-to-end로 최적화 될 수 있습니다.

이렇게 통합된 구조는 매우 빠르다고 하는데, 저자는 YOLO는 localization 에러를 더 많이 만들지만 background에 대한 false positive는 덜 예측하기때문에 결과적으로 다른 방법들보다 성능이 뛰어나다고 말하고 있습니다.

## Introduction

최근의 방식인 R-CNN은 region proposal을 사용하여 제안된 boxe들에 대해 classifier가 수행되는 방식입니다.

분류 이후에 bbox를 정제하고, 중복되는 detection제거, 다른 object에 기반해 box의 점수를 rescore하는 방식입니다. 이러한 pipline은 각각의 요소가 분리되어서 학습되기 때문에 느리고 최적화하기 어렵다는 점이 있습니다.

본 논문에서는 object detection을 single regression으로 재구성하고, 이미지의 픽셀에서 bbox좌표, 클래스 확률까지 한번에 이어지도록 하였습니다. 객체가 무엇이고 어디에 있는지 예측하기위해 이미지를 단 한번만 보기때문에 You Only Look Once(YOLO)라고 하였습니다.

![](.\image\Yolov1_fig1.png)

YOLO의 system은 다음과 같습니다.

1. input image를 448\*448로 조정
2. 이미지에 대해 single convolutional network실행
3. NMS와 결과 detection의 threshold값 설정

위의 과정과 같이 YOLO는 진행됩니다.

여기서 사용된 single convolutional network는 동시에 여러개의 bbox들을 예측해주고 box들에 대한 클래스 확률까지 예측하게 됩니다. 다시말해서 YOLO는 전체 이미지에 대해 학습하고 detection 성능을 직접 최적화 합니다.

이렇게 통합된 모델의 장점을 전통적인 방법에 비교하여 몇가지 소개하고 있습니다.

1. **속도가 매우 빠르다!**

detection을 regression문제로 구성하였기 때문에 복잡한 pipline이 필요가 없어졌습니다. 배치없이 Titan X GPU에서 45fps였으며, 더 작은 버전의 경우 150fps가 나왔습니다.

이것은 25ms미만의 지연으로 real-time으로 적용할 수 있는 것을 의미합니다. 게다가 다른 real-time 시스템에 비해서 두 배가 넘는 mAP를 달성하였습니다.

2. **예측할때 이미지 전체적으로 추론한다!**

sliding window와 region proposal 기반의 기술과 다르게 YOLO는 학습과 테스트시에 전체적인 이미지를 보기때문에 외관과 같은 클래스에 대한 정보를 encodes합니다.

Fast R-CNN의 경우 커다란 context를 보지 못하기 때문에 object를 background라고 인식하는 경우가 생깁니다. YOLO는 그러한 경우를 절반 이하로 발생시켰습니다.

3. **객체의 일반적인 표현을 학습한다!**

자연 이미지로 학습하고 예술 이미지로 테스트하였을때, YOLO는 R-CNN과 같은 방법보다 큰 차이가 있습니다. 또한 YOLO는 일반화 되기 때문에 예상치 못한 입력에 대해서도 실패할 가능성이 적습니다.

YOLO는 SOTA를 달성한 시스템의 정확도에 미치지는 못하지만 물체를 매우 빠르게 식별할 수 있습니다.

## Unified Detection

YOLO의 input image는 S\*S grid로 나눕니다. 

object의 중앙이 grid cell에 들어가게 되면, 그 grid cell은 object를 detect하는데 responsible하다고 합니다.

> responsible: grid cell 안에서 하나의 object에 대해 하나의 bbox를 대응시키는 것
>
> ![](.\image\yolov1_responsible.png)
>
> [그림출처](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)
>
> 위의 그림처럼 두개의 bbox를 예측하였다고 했을때, 두 박스 모두 객체를 탐지하였지만, 더 큰 IOU를 갖는 bbox에 대해서 responsible하다고 한다.
>
> responsible이면 1, not responsible이면 0

각 grid cell은 B개의 bbox와 box안의 object를 confidence점수를 통해 예측하게 됩니다.

이때 사용되는 공식은 $P_r(object)*IOU^{truth}_{pred}$입니다. 즉, cell 안에 객체가 없다면 점수는 0점이 됩니다.

각각의 bbox는 총 5개로 구성되어있습니다. (x, y, w, h, confidence)

> x, y: box의 중심좌표
>
> w, h: 전체 이미지에 대한 높이와 너비
>
> confidence: confidence score

각 grid cell은 C개의 conditional class probabilities를 $P_r(Class_i|Object)$로 예측합니다. 이것은 box의 개수에 상관하지 않고 grid cell당 하나의 class probabilities를 예측하게 됩니다.

논문에서 PASCAL VOC를 사용하였을때, S=7, B=2, C=20 를 사용하여 7\*7\*30개의 tensor를 예측하였습니다.


![](.\image\yolov1_fig2.png)

### Network Design

![](.\image\Yolov1_fig3.png)

GoogLeNet으로부터 영감을얻어 24개의 conv layer와 2개의 fc layer로 이루어져 있습니다.

또한 inception 모듈대신 YOLO에서 간단히 1\*1의 reduction layer를 사용하였으며, 그 후에 3\*3의 conv layer를 사용하였습니다.

### Training

training을 진행하기 전에 다음과 같은 작업을 진행해 주었습니다.

1. 24개의 conv layer에서 20개의 conv layer + pooling layer + fc layer를 ImageNet 1000-class dataset으로 pretrain
2. input을 224\*224 에서 448\*448로 향상시키기
3. pooling layer + fc layer를 제거하고 4개의 conv layer + 2개의 fc layer 추가, 이때 추가된 layer의 weight는 random하게 초기화
4. 마지막 layer에서 bbox와 coordinate예측
   - bbox의 w, h를 0~1 사이로 normaliza
   - x, y좌표는 offset을 사용해 0~1로 조정
5. 마지막 layer는 linear activation function사용, 다른 layer는 Leaky ReLU사용

![](.\image\yolov1_s1.png)

bounding box coordinates 예측이 box와 object를 포함하지 않는 것을 막기위해 $\lambda_{coord}, \lambda_{noobj}$를 사용하였습니다.

일반적으로 예측하는 box에 object가 없는 경우가 많기 때문에 object가 없는 background class를 찾는것이 더 효율적으로 판단 한 이유입니다.

![](.\image\yolov1_loss.png)

논문에서 사용한 loss function입니다.

이것은 Location of Bounding Box Loss + Size of Bounding Box Loss + Object Loss + Classification Loss를 합친 공식입니다. 

이 식을 더알아 보기 쉽게 하면 다음과 같습니다.

![](.\image\yolov1_loss2.png)

공통적으로 사용하는 parameter는 다음과 같습니다.

- S: Image의 가로 세로 grid cell로 나눌 갯수, S^2은 모든 grid cell
- B: number of bounding box
- $1^{obj}_{i}$: object가 존재하는 grid cell i
- $1^{obj}_{ij}$: obejct가 존재하는 grid cell i의 bounding box predictor j
- $1^{noobj}_{ij}$: object가 존재하지 않는 grid cell i의 bounding box predictor j
- $\lambda_{coord}$: coordinate에 대한 loss와 다른 loss들과의 균형을 위한 parameter, 논문에서 5로 정의
- $\lambda_{noobj}$: object가 있는 box와 없는 box간의 균형을 위한 parameter, 논문에서 0.5로 정의



검은색 부분은 box에 대한 loss입니다. 이를 sum-squared error를 통해 구하고 있습니다.

근데 검은색의 두번째 식은 x, y가 아닌 w, h를 이용하였는데, SSE는 box의 크기에 상관없이 동일한 weight를 갖고있고, 큰 box보다 작은 box에서 적은 편차를 갖고 있는 것이 중요합니다. 즉, object를 찾는 box의 크기가 작을수록 좋기때문에 큰box와 작은 box의 편차를 줄이고자 w, h의 제곱근을 사용하여 계산하였습니다.

파란색의  식은 confidence score에 대한 loss와 object가 없는 box대한 loss입니다.

각 grid cell에서 object가 존재할 확률 $P_r(object)$를 구합니다.

![](C:\workspace\논문\image\yolov1_loss_1.png)

마지막 노란색의 식은 cell에서 class를 예측한 loss입니다.

object가 존재하는 grid cell i에서 실제 class일 확률을 구하였습니다.

![](.\image\yolov1_loss_2.png)

> 더욱 자세한 설명은 https://taeu.github.io/paper/deeplearning-paper-yolo1-02/ 참조

논문에서 추가적으로 classfication error는 object가 존재하는 grid cell에 의해 영향을 받으며, bounding box coordinate error는 ground truth box 에 의해서만 영향을 받는다고 설명하고 있습니다.

network의 추가적인 구성은 다음과 같습니다.

- batch = 64
- momentum = 0.9 / decay = 0.0005
- learning rate = 0.001, epoch이 진행할 수록 최대 0.01까지 상승
  - 75 epoch: 0.01
  - 30 epoch: 0.001 / 0.0001
- dropout = 0.5

grid 디자인을 통해서 각 물체에 대해 하나의 box를 예측하였으며, NMS를 통해 2~3%의 mAP향상이 있었습니다.

### Limitation of YOLO

grid cell이 하나의 클래스만 예측하기 때문에 작은 object가 주변에 있으면 제대로 예측하는 것이 어렵습니다.

즉, grid cell에 하나의 클래스만 가질수 있기 때문에 bbox prediction에 강한 공간적 제약을 줍니다. 이것은 모델이 예측할 수 있는 물체 수를 제한 하는 것과 같은 것이기 때문에 새떼와 같은 object에 대해 어려움이 있습니다.

또한 새로운 형태의 bounding box의 경우 예측하기가 어렵습니다.

학습 데이터로부터 bounding box를 예측하는 것을 학습하기 때문에 비주류 형태의 object를 generalize하는 것이 어렵습니다.

마지막으로 localization에 대해 부정확한 경우입니다.

loss function은 작은 box와 큰 box에 대해 동일하게 처리하기 때문에, 작은 box의 경우 IOU에 큰 영향을 주게 됩니다. 이것이 localization에 부정확하게 되어 오류를 발생시키게 됩니다.

## Experiments

먼저 PASCAL VOC 2007 dataset을 사용하여 detector의 속도와 성능을 비교하는 표 입니다.

![](.\image\yolov1_table1.png)

Fast YOLO가 가장 빠르고 두번째로 높은 mAP를 나타내는 것을 확인할 수 있으며, 가장 정확한 것은 YOLO로 Fast YOLO보다 10mAP가 더 높게 나오고 있습니다.

![](C:\workspace\논문\image\yolov1_fig4.png)

VOC 2007의 error를 분석한 것입니다.

다양한 카테고리에 대한 localization, background error rate를 나타냅니다.

Fast R-CNN이 localization의 오류가 8.6%이며 YOLO는 19%로 Fast R-CNN이 localization error는 작은 것을 확인할 수 있습니다.

하지만 background error의 경우 13.6%와 4.75%로 이 부분에서는 YOLO의 error가 더 적은것을 확인할 수 있었습니다.

![](C:\workspace\논문\image\yolov1_table2.png)

Fast R-CNN의 background error와 YOLO의 localization error를 보완하고자 두개를 합쳐 실험을 진행한 결과입니다. 75.0의 mAP로 높은 mAP를 기록하였으나 속도측면에서는 도움이 되지 않았다고 합니다.

![](C:\workspace\논문\image\Yolov1_fig6.png)

test시에 artwork를 사용한 결과입니다.

픽셀 레벨은 다르지만 object의 크기나 모양이 유사하기 때문에 YOLO가 좋은 detection을 예측할 수 있는 것을 확인할 수 있었습니다.

