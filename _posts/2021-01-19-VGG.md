---
layout: post
title: VGG
data: 2021-01-19
excerpt: "VGG"
tags: [VGG, network]
coments: false
mathjax: true
---

# VGG

논문의 제목은 VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION입니다.

이미지 인식에서 네트워크의 깊이가 정확도에 어떤 영향을 미치는지에 대해 연구하였습니다.

연구팀은 ConvNet의 convolution layer를 증가시키며 깊이(depth)를 중요하게 생각하고 연구하였습니다. 이때 모든 layer에는 3\*3크기의 filter를 적용하였으며, 이 결과로 이미지 인식에서 좋은 결과를 얻게 되었습니다. ImageNet Challenge 2014에서 localisation과 classification분야에서 각각 1위와 2위를 달성하였습니다.

## ARCHITECTURE

input 으로 224\*224 크기의 RGB이미지를 넣어줍니다. 이미지 전처리 작업으로는 RGB값을 평균내어 빼주는 작업만 진행하였습니다.

convolution layer에 3\*3크기의 filter를 적용하였다고 합니다. 3\*3크기의 filter를 사용한 이유는 이 크기가 left, right, up, down, center를 고려할 수 있는 최소한의 크기이기 때문이라고 합니다. 또한 1\*1크기의 filter도 사용하였습니다.

이 filter들의 stride는 1을 사용하였으며, padding도 1로 설정하였습니다.

pooling layer는 convolution layer를 다음으로 적용하였으며, 총 5개의 max pooling layer로 구성하였습니다. 이때의 크기는 2\*2이며 stride는 2입니다.

또한 3개의 fully-connected layer를 사용하였습니다. 첫번째와 두번째 layer는 4096개의 채널을 갖고있으며 세번째의 layer는 classification을 위한 1000개의 채널을 갖고있습니다. 마지막으로 출력된 것으로부터 소프트맥스를 적용하여 줍니다.

그 외의 layer에는 ReLU를 사용하였습니다. 

여기에서는 AlexNet에서 사용한 LRN(local response Normalisation)을 사용하지 않았습니다. 그 이유는 성능향상에 도움되지 않았으며 메모리 사용 및 연산 시간만 늘어났기 때문이라고 합니다.

11 layer(8 conv, 3 FC)인 A구조부터 19 layer(16conv, 3 FC)인 E구조까지 존재합니다. conv layer의 채널수는 64채널부터 시작하여 max-pooling layer를 통해 2제곱씩 커지게 됩니다. 따라서 최대 512채널까지 커지게 됩니다.

논문에서는 depth가 늘어났는데도 더 커다란 conv layer를 사용한 얕은 신경망보다 파라미터 수가 줄었다고 말하고 있습니다.

3\*3filter 2개를 사용하는 것은 5\*5filter를 한개 사용한 것과 동일한데 3\*3크기의 filter를 여러개 사용한 이유로 작은 사이즈를 여러번 나누어 사용하게 되면 ReLU를 더 많이 통과해 non-linear한 결정을 더 잘하게 된다고 말하고 있습니다. 또한 파라미터의 수가 감소되는 효과를 볼 수 있습니다.

C구조에서는 1\*1의 filter를 추가적으로 사용하여 non-linear한 정도를 더 강화 시켰습니다.

![](.\image\VGG_table.jpg)

## TRAINING

train을 진행하는데 다음과 같은 파라미터를 사용하였습니다.

- multinomial logistic regresion objective = cross entropy

- batch size = 256

- momentum = 0.9

- L2(L2 regularlisation) = 5\*10^-4 & dropout = 0.5

- learning rate = 10^-2

  > validation error가 증가하면 learning rate를 1/10씩 감소시켰습니다.
  >
  > 총 3번의 learning rate가 감소되었습니다.

74에폭을 반복 후 learning이 멈췄습니다.

더 많은 파라미터와 더 깊은 구조인데도 불구하고 AlexNet보다 적은 에폭을 기록하였습니다. 그 이유로는 implicit regularisation과 pre-initialisation으로 설명하고 있습니다.

implicit regularisation은 앞에서 설명한 한개의 7\*7의 filter를 사용하는 것보다 3개의 3\*3의 filter를 사용하는 것이 더 적은 파라미터를 사용한다는 것을 말합니다.

pre-initialisation은 먼저 A 모델을 학습시키고 이때 학습된 layer들을 바탕으로 다른 모델들에 사용한 방법입니다. A 모델에서 처음 4개의 convolutional layer와 마지막 3개의 fully-connected layer를 사용하였습니다.

모델의 input의 크기는 224\*224이기 때문에 이미지를 rescale한 후 랜덤으로 crop하여 사용하였습니다.

rescale할때 aspect ratio를 유지하며 진행해주었는데 이러한 방식을 isotropically-rescaled라고 설명하고 있습니다.

논문에 S값이 등장하는데, S값은 사이즈를 줄이기 위한 최소값이라고 생각이 됩니다. 따라서 이미지에서 제일 작은 side를 S값으로 설정하여 rescale를 진행합니다.

S값을 설정하는데 두 가지 방법을 설명하고 있습니다.

1. Single-scale training

이 방법은 S를 256이나 384로 고정시키는 것입니다. S가 384인 경우 학습 스피드를 증가시키기 위해 S가 256으로 설정해 학습시킨 값들을 바탕으로 다시 학습 시켰습니다. 또한 learning rate를 줄이고 학습시켰는데, S가 256일때 많은 학습이 진행되었기 때문이라고 말하고 있습니다.

2. Multi-scale training

이 방법은 S를 고정시키지 않고 256~512까지 랜덤하게 설정하는 것입니다.이미지들이 모두 같은 사이즈가 아니기 때문에 이렇게 random하게 학습을 시키면 학습효과가 더 좋다고 설명하고 있습니다. 이를 scale jittering이라고 말합니다.

이 방법을 사용할 때 속도상의 이유로 S가384로 pre-trained된 single-scale을 모델로 fine-tuning을 합니다.

## TESTING

test할때도 train할때와 마찬가지로 rescale해줍니다. 이때는 Q라고 부릅니다. 이 Q는 S와 같을 필요는 없으며 S와 Q가 다르면 모델의 성능이 좋아진다고 말하고 있습니다.

또한 validation할때와 train할때의 구조가 달라집니다. validation할때는 FC layer를 conv layer로 바꿔줍니다. 첫번째 FC layer를 7\*7 conv layer로, 마지막 두개의 FC layer를 1\*1 conv layer로 바꿔 적용합니다. 이를통해 전체 이미지에(uncropped image)적용시킬 수 있다고 합니다. 

train에서는 crop방법을 사용하였습니다. 하지만 test에서는 FC layer가 1\*1 conv layer로 바뀌어서 uncropped image를 사용해도 된다고 하는데, 이 이유는 다음과 같습니다.

먼저 FC layer가 1\*1conv layer와 대치되는 이유는 이전 데이터의 모든 노드가 다음 레이어의 모든 노드와 연결되기 때문입니다. 그리고, FC layer는 MLP개념으로 입력노드가 하이퍼파라미터로 정해져 있지만 conv연산에서는 이것이 상관 없기 때문에 uncropped image를 사용해도 되는 이유입니다.

softmax 전의 feature map size는 이미지 크기에 따라 달리진다고 합니다. 즉, 1\*1conv filter를 통과하는데 7\*7크기의 output feature map의 크기를 얻을 수도 있다는 것입니다. 이러한 output feature map을 class score map이라고 합니다. 이렇게 얻은 feature map은 spatially averaged(mean or average pooling)을 하게 됩니다.

이후 filpped image와 original image의 평균값을 통해 최종score를 출력한다고 말하고 있습니다.

이 모델들을 4개의 NVIDIA Titan Black GPU를 통해 2~3주가 걸렸습니다.

## 실험결과

사용한 데이터로는 ILSVRC 2012를 사용하였고, top-1과 top-5 error방식으로 성능을 측정하였습니다.

top-1은 multi-class classification error이고, top-5는 ILSVRC에서 요구하는 기준을 사용하였습니다. 또한, validation set을 test set으로 사용하였습니다.

Local Response Normalization을 사용한 모델(A-LRN)과 사용하지 않은 모델(A)에서의 성능차이가 나타나지 않아 모델B부터는 LRN을 사용하지 않았습니다.

1\*1크기의 conv filter를 사용하는 모델C보다 3\*3크기의 conv filter를 사용한 모델D가 성능이 더 좋게 나왔습니다. 이는 3\*3크기의 conv filter가 spatial context(공간 형태?)를 더 잘 추출하기 떄문입니다.

평가방식은 single-scale과 multi-scale두 방식으로 나누었습니다.

single-scale은 train에서와 비슷하게 test시 image의 크기를 고정하는 것입니다. S=Q인 256 또는 384로 고정하여 사용하였습니다.

![](.\image\VGG_table2.jpg)

multi-scale방식은 하나의 S 사이즈에 대해서 여러가지 Q사이즈로 평가를 진행 하였습니다.

![](.\image\VGG_table3.jpg)

최종으로 ILSVRC-2014에 다양한 classification 모델과 대결한 결과를 보여주는데요 VGG는 여기서 top-5 error가 6.8%로 2등을 하게 됩니다.

![](.\image\VGG_table4.jpg)