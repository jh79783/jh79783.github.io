---
layout: post
title: DETR(DEtection TRansformer)
data: 2021-06-21
excerpt: "DETR"
tags: [transformer]
coments: false
mathjax: true
---

# DETR(DEtection TRansformer)

- 제목: End-to-End Object Detection with Transformers
- 저자: Nicolas Carion, Francisco Massa 외 4인 Facebook AI
- 인용수: 477회
- 학술지: ECCV 2020

## Introduction

- Object detection은 바운딩 박스와 카테고리 라벨의 짝을 맞추는 것
- 최근의 detector들은 이를 proposal, anchors, window centers를 사용한 간접적인 방법으로 해결
- 이러한 방식은 nms나 anchor generate같은 사람의 손이 거쳐야 하는 부분에서 성능이 영향을 받음
- 따라서 본 논문에서는 이를 transformer 기반의 encoder-decoder 구조를 사용하여 해결
- bipartite matching(이분 매칭) loss를 설계하여 hand-designed작업 없는 end-to-end의 간단한 pipline을 나타냄
- 또한 train data에 없는 형태의 object가 test data에 등장하였을 때, 이것에 대해 유연하게 대응할 수 있음

## Related work

### Set Prediction

- 하나의 이미지에서 여러 객체를 예측하려다 보니 중복된 예측이 나옴

- 기존 detector는 NMS를 사용하여 해결

- 본 논문에서는 이를 bipartite matching을 사용하여 해결

  > RNN을 사용한 다른 detector가 있지만 명렬적으로 decoding하는데 이점이 없음

## The DETR model

- set prediction에서는 다음과 같은 두 개의 중요한 요소가 있음

  1) a set prediction loss that forces unique matching between predicted and ground truth boxes

  2) an architecture that predicts (in a single pass) a set of objects and models their relation

### Object detection set prediction loss

- DETR은 N개의 fixed-size set으로 prediction함

  > N(=100: query)은 이미지의 object 개수보다 꽤 크게 설정

- loss는 GT와 prediction사이의 최적의 bipartite matching을 만들어내고, 이를 통해 bounding box loss를 최적화 함

![](.\detr_s1.png)

> $$
> y=GT\\
> \hat{y}=N개의\space prediction\space set
> $$
>
> 이때 N은 이미지의 object 수 보다 많아지기 때문에 y가 N크기가 되도록 $\phi$(no object)를 추가

- GT와 prediction 사이의 bipartite matching을 찾기 위해 matching cost가 최소가 되도록 매칭

  > transformer의 decoder가 예측하는 객체의 class가 GT에 포함 될때, loss가 낮아짐

$$
L_{match}=y_i\hat{y}_{\sigma(i)}=-1_{\{c_i!=\phi\}}\hat{p}_{\sigma_{i}}(c_i)+1_{\{c_i!=\phi\}}L_{box}(b_i,\hat{b_\sigma}_{(i)})
$$

> 앞의 -1은 해당 class probability가 높을 수록 loss를 낮게 하기 위해서
>
> 이때는 matching cost loss를 계산하는 것(?)
>
> 즉, 1:1 매칭에 대한 loss이기 때문에 중복 예측이 생기지 않음

- 매칭된 set에 대해 Hungarian algorithm에 기반한 Hungarian loss를 통해 학습

![](.\detr_s2.png)

> 진짜로 GT와 prediction 사이의 loss(?)
>
> $\sigma(i)$= prediction index
>
> $c_i$= target class label
>
> $b_i$=GT box(center, height, width) [0, 1]의 4차원
>
> $\hat{p}_{\sigma_{(i)}}(c_i)$= $c_i$에 대한 class probability
>
> $\hat b(\sigma_{(i)})$=predict box

- 이때, no_object에 대해서도 학습을 진행
- no_object가 굉장히 많기 때문에 class imbalance가 발생하기 때문에 no_object일때 weight에 1/10을 줌

### Bounding box loss

- 다른 detecotr는 초기 설정한 값에 대해 조절하는 정도를 학습

- DETR은 bounding box를 directly predict

- 이때, 사용한 loss로 L1 loss에 GIoUloss 더해주었음

  > L1 loss는 작은 객체와 큰 객체의 차이가 비슷함에도 scale이 다양해지기 때문에 

$$
\lambda_{iou}L_{iou}(b_i,\hat b_{\sigma(i)})+\lambda_{L1}||b_i-\hat b_{\sigma(i)}||_1
$$

## DETR architecture

![](.\detr_fig2.png)

- 전체적인 구조는 fig.2와 같음

- backbone으로 ResNet을 사용하였는데, 여기서 image의 feature를 추출하였음

- 그리고 위치 정보를 담기위해 positional encoding을 추가하게 됨

- transformer의 encoder-decoder구조를 사용

- 1\*1convolution을 사용해 spatial한 정보는 고정

- encoder에 들어가기 위해서 H, W를 합쳐 2차원으로 만듬(vector)

- 병렬적으로 encoding을 수행해 마지막 encoder에서 decoder로 넘어가게 됨

- encoder의 input으로 image feature map을 받아 encoding 수행

  > input: d\*HW 크기의 feature map
  >
  > d: 벡터 사이즈
  >
  > HW: sequence 개수
  >
  > 픽셀에 대한 연관성을 학습(위치)
  >
  > output: key, value

- decoder에서 N개의 query를 입력으로 받아 decoding 수행

  > input: N개의 object query와 encoder의 key, value
  >
  > object query를 병렬적으로 학습(?)
  >
  > output: N개의 object

- FFN에서 bipartite matching을 통해 적절한 matching set을 찾고, object별로 box를 최적화하는 loss가 계산됨

- 따라서 최종적으로 bounding box의 center, height, width의 4차원 정보와 class를 출력하게 됨

  > FFN은 perceptron으로 3층의 layer로 activation은 ReLU를 사용
  >
  > boundary box와 class를 출력
  >
  > softmax를사용해 class label예측
  >
  > class와 box coordinates가 tuple로 prediction이 출력됨

![](.\detr_fig10.png)

## Experiments

- COCO 2017 detection datset을 사용

  > 평균적으로 7개의 instance가 존재
  >
  > 최대 63개의 instance가 존재

- Optimizer: AdamW

- inital lr: $10^{-4}$

- backbone lr: $10^{-5}$

- weight decay: $10^{-4}$

- use backbone:

  - ImageNet으로 pretrain한 ResNet-50, ResNet-101

    > 이들을 DETR, DETR-R101로 부름

  - backbone의 Conv5 layer의 stride를 삭제하여 resolution을 증가

    > 이 모델을 DETR-DC5로 부름

  - Scale agumentation을 사용

    - shortest side를 기준으로 480~800pixel, longest side를 기준으로 1333pixel까지 resize

  - Random Crop agumentation을 사용하였을 때 1AP의 증가 효과가 있음

  - transformer에서 default dropout으로 0.1을 사용

- 16개의 V100 GPU를 사용하여 300 epoch 학습(약 3일 소요)

  > batch: 4

- 비교는 500epoch으로 학습한 Faster R-CNN을 사용(+는 3배의 시간을 더 학습)

![](.\detr_table1.png)

- Faster R-CNN보다 AP가 높게 나오지만 small object에 대해서는 성능이 떨어짐

  > hungarian loss self-attention구조가 small object를 검출하는데에 적합하지 않기 때문

![](.\detr_fig3.png)

- Encoder의 마지막 layer에서의 시각화
- instance를 잘 구분하는 것을 확인

![](.\detr_fig4.png)

- Decoder layer와 NMS의 유무에 따른 성능

![](.\detr_fig6.png)

- Decoder의 마지막 layer에서의 시각화
- bonding box를 위한 가장자리 부분이 잘 활성화 되는 것을 확인

![](.\detr_table4.png)

- box loss에 따른 성능 비교