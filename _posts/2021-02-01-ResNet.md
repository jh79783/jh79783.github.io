---
layout: post
title: ResNet
data: 2021-02-01
excerpt: "ResNet"
tags: [ResNet, network]
coments: false
mathjax: true
---

# ResNet

논문의 제목은 *Deep Residual Learning for Image Recognition*이며 IEEE학술지의 CVPR2016에서 공개되었습니다.

저자는 Kaiming He 외3명이며 microsoft에서 개발한 네트워크로 약6만8천회가 인용되었습니다.

# Abstract

네트워크가 깊어질수록 학습시키것이 어렵기때문에, 깊은 네트워크에서도 쉽게 훈련시키기 위해 residual learning framework 방법을 제안합니다.

ResNet은 총 152층의 layer를 쌓았으며 이를 ImageNet 데이터셋을 통해 테스트하였는데, top-5 error rate가 3.57%로 ILSVRC 2015에서 1위를 차지하게 되었습니다. 또한 COCO 데이터셋을 통해 28%의 성능향상이 있었다고 합니다.

# Introduction

ImageNet을 통한 결과는 모두 depth가 16~30정도인 모델(very deep model)을 사용하였으며, 다른 visual recognition에서도 사용하여 큰 도움을 얻었다고 합니다.

네트워크가 깊어짐에따라 발생하는 vanishing/exploding gradient 문제는 normalized initialization과 intermediate normalization을 통해 대부분 해결하였으나, 정확도가 포화상태에 다다르게 되면 성능이 급격하게 떨어지는 또다른 문제가 나타나게 되었습니다. 이러한 문제는 overfitting으로 인해 발생하는 것이 아니고, 더많은 layer가 추가되면 더 높은 training error를 나타낸다는 연구결과가 아래 사진으로 나타나있습니다.

![](.\resnet_fig1.png)

> 20-layer와 56-layer가 CIFAR-10 데이터셋을 학습하였을때의 error rate

위의 결과보다 더 깊은 architecture에서도 SGD(stochastic gradient descent)와 같은 다양한 solution을 사용하여 문제를 해결할 수 있으나 커다란 효과를 기대하기는 어렵다고 합니다.

따라서 논문에서는 *Deep Residual Learning Framework*를 도입하여 위와같은 성능저하 문제를 해결하고자 합니다.

# Deep Residual Learning

기존에는 input(x)을 목표(y)로 mapping하는 함수 H(x)를 찾는 것인데, 이 논문에서는 출력과 입력의 차를 얻게 학습을 시키는 H(x) - x를 제시합니다. 이를 잔차(residual)라 부릅니다. 따라서 F(x) = H(x) - x 가 되므로 결과적으로 H(x) = F(x) + x가 됩니다.

![](.\resnet_fig2.png)

저자들은 단순히 H(x) = x가 되도록 학습하는 것보다 F(x) = 0이 되도록 학습하는 것이 더 쉬울것으로 생각하였기 때문에 H(x) = F(x) + x식을 사용합니다.

논문에서 오른쪽 그림을 identity shortcut connection이라고 말하고 있습니다. 이런 connection과 같이 x를 더해주어 네트워크의 output을 0으로 만들어 최종 output이 x가 되도록 학습시키는 것입니다. 또한 이렇게 해주어도 연산량에는 큰 차이가 없습니다.

H(x)가 x가 되도록 학습하면 미분을 해도 x의 미분값으로 1을 갖게되어, layer가 아무리 깊어진다하더라도 최소 gradient가 1이상의 값을 가지기 때문에 vanishing 문제를 해결할 수 있습니다.

이를 이용한 네트워크는 SGD에 의한 역전파로 end-to-end학습이 가능하고, 일반적인 라이브러리를 사용해 쉽게 구현할 수 있다고 합니다.

위의 식은 저자들이 가정한 것이기 때문에 실제로 학습에 도움이 되는지 의구심이 듭니다. 논문에서 입력 x가 학습 시에 가이드로 작용하여 학습에 도움을 주기 때문에 이것은 긍정적인 효과를 기대할 수 있다고 합니다.

논문에서 FC layer에서의 building block에 대한 공식을 다음과 같이 정의했습니다.

$$
y = F(x,{W_j}) + x\\
F=W_2\alpha(W_1x)
$$

> input: x
>
> output: y
>
> learning residual mapping: F

아랫식을 간소한 것이 위의 식입니다.

$F(x,{W_j})$식이 나타내는 의미는 residual mapping을 나타내게 됩니다.

아랫식에서 $\alpha$는 ReLU를 의미하며 bias는 간소화를 위해 생략하였습니다.

F + x연산을 하기위해 둘의 차원수(dimension)가 같아야 합니다. 따라서 이를 수행하기 위한 공식은 다음과 같이 정의됩니다.
$$
y=F(x,{W_j}+W_sx)
$$
여기서 $W_s$는 단순히 dimension을 같게만들어 주기위한 용도로만 사용됩니다. 이 식을 projection shortcut connection이라 부릅니다.

위의 식은 간소화를 위해 FC에서 표현한 것인데, 이것을 Conv에서도 구현할 수 있습니다.

이때는 dimension을 맞춰주기 위해 1\*1 filter를 사용하게 됩니다.

# Network Architectures

논문에서는 Plain network와 Residual network에 대해서 테스트를 진행하였습니다. 사용한 데이터셋으로는 ImageNet을 사용하였습니다.



![](.\resnet_table1.png)

### Plain network

VGG를 참고하여 만든 네트워크입니다.

모두 동일한 feature map을 갖게위해 layer는 동일한 수의 filter를 갖게 합니다. 또한, feature map사이즈가 절반이 된다면 filter수는 2배가 되도록 만들었습니다.

Conv layer는 3\*3의 filter를 stride는2로 사용하고, downsampling, global average pooling layer를 사용, 마지막엔 softmax를 사용하였습니다.

최종적으로 레이어는 34개로 VGG에 비해 18%정도의 연산을 합니다.

### Residual Network

Plain 네트워크에 shortcut connection을 도입한 것입니다. 위에서 말한 것처럼 input 과 output을 차원으로 맟춰주어야 합니다.

차원이 동일한 경우 직접사용하며, 차원이 증가한 경우는 zero-padding을 합니다. 이때, 추가적인 parameter는 없습니다. 또한, 위에서 설명한 식의 아랫 식을 사용합니다.

VGG의 경우 196억 FLOPs, plain와 residual의 경우 36억 FLOPs로 동일합니다.



# Implementation

ImageNet 데이터에 대한 실험은 AlexNet과 VGG에서 사용한 방법을 따라하였습니다.

- initialize weight
- SGD, mini batch = 256
- learning rate = 0.1
- momentum = 0.9
- no dropout

# Experiments

ImageNet 2012 classification 데이터를 사용하였습니다. train/validation/test에 사용된 이미지의 개수는 각각 1.28M/50K/100K입니다.

평가 항목으로는 top-1과 top-5 error를 평가하였습니다.

![](.\resnet_fig3.png)

18 layer와 34 layer에 대한 Plain과 ResNet에 대해 실험한 결과입니다.

Plain 네트워크의 경우 34 layer의 error rate가 더 높은 것을 확인할 수 있으며, ResNet의 경우는 34 layer의 error rate가 더 낮은 것을 확인할 수 있습니다.

또한 ResNet이 더 빠르게 수렴하는 것을 볼 수 있습니다.

![](.\resnet_table2.png)

연구팀은 여러가지 설정을 다르게 하여 비교해 보았습니다.

ResNet의 A, B, C는 옵션을 다르게 준 것인데, B에 zero-padding을 사용하였고, C에는 projection shortcut을 사용하였습니다. 하지만 차이가 매우 작기 때문에 projection shortcut이 성능저하 문제를 해결하는데 필수적이지 않는 다는 것을 알게되어 memory소모를 줄이기 위해 앞으로는 이것을 사용하지 않았습니다.

연구팀은 이것보다 더욱 deep한 architecture를 만들었으며, 학습에 걸리는 시간을 고려해 기본 구조를 다음과 같이 변경하였습니다.

![resnet_fig4](.\resnet_fig4.png)

이를 Depper Bottleneck Architecture이라 이름을 붙였습니다.

먼저 1\*1을 통해 GoogLeNet의 Inception구조처럼 dimension을 줄이기 위해 사용하였으며 이것을 3\*3 conv를 수행 후, 마지막 1\*1 conv를 통해 dimension을 확장시키게 됩니다. 

![](.\resnet_fig5.png)

위 구조를 반영한 결과입니다. top-5 error rate는 ResNet 152 layer가 4.49%까지 떨어지게 되었습니다.

![](.\resnet_fig6.png)

최종적으로 ILSVRC 2015에 제출한 모델은 2개의 152 layer를 조합하여 사용하였으며, 3.57%로 1위를 차지하였습니다.