---
layout: post
title: YOLOv2
data: 2021-04-11
excerpt: "YOLOv2"
tags: [yolo, detector]
coments: false
mathjax: true
---

# YOLOv2

- 제목: YOLO 9000: Better, Faster, Stronger
- 저자: Joseph Redmon 외 1명
- 인용: 7543회

- 성능
  - 67 FPS
  - 76.8 mAP

## Better

기존 YOLO는 localization error와 낮은 recall을 보여주었기 때문에 이를 개선할 방법을 소개합니다.

다른 network는 정확도를 높이기 위해 더 큰 network를 만들게 됩니다.

하지만 저자는 정확도 뿐만아니라 속도까지 향상시키는 것이 목표이기 때문에 network를 확장하는 것이아닌 단순화한 후 train하도록 만들었습니다.

![](.\image\yolov2_table2.png)

### Batch Normalization

YOLOv1에서 사용한 모든 conv layer에 batch normalization을 적용시켰습니다. 

이를 사용하여 mAP가 2%이상 향상된 효과를 보았습니다. 또한 drop out layer를 제거할 수 있었습니다.

### High Resolution Classifier

학습할때는 224\*224의 해상도로 학습을 진행하지만, detection을 수행할 때는 448\*448로 수행합니다. 

따라서 큰 해상도에 대해 fine-tuning을 하여 약4% mAP가 증가하였습니다.

### Convolutional With Anchor Boxes

YOLOv1은 x,y,w,h값을 직접 최적화 된 값으로 찾아가게끔 학습을 진행하였습니다. Faster r-cnn에서는 anchor box를 통해 offset과 confidence score를 계산하면 되는 구조이기 때문에 이것이 학습하는데는 훨씬 간단하게 할 수 있습니다.

따라서 YOLOv2에서는 이와 비슷하게 anchor box를 미리 정의하고 사용합니다.

이를 위해 RPN과 비슷하게 마지막단의 fc layer를 convolution layer를 사용하였습니다.

또한 detection단계에서 network의 input을 448에서 416으로 축소하였는데, result feature map을 홀수로 출력하기 위함입니다.

> 홀수일때 gride cell이 feature map의 중앙부분에 위치하게 됨
>
> 대부분의 물체는 중앙에 위치하기 때문에 효율성이 높아지게됨

416\*416의 input을 사용하여 13\*13의 result feature map을 얻었습니다.

결과적으로는 anchor box가 없으면 정확도에서의 약간의 감소만 있으며, recall의 경우 더 높은 값을 나타내었습니다.

> no anchor box
>
> - recall: 81%
> - mAP: 69.5
>
> use anchor box
>
> - recall: 88%
> - mAP: 69.2

### Dimension Clusters

anchor box를 지정하는데, 직접 지정하여 사용하는것이 문제라고 판단하였습니다. 따라서 dataset에 있는 GT bboxes에 k-means clustering을 사용해 최적의 anchor box를 찾도록 하였습니다.

원래 k-means clustering은 'euclidean distance'를 사용합니다.

> ex)
>
> step1. k=3이라고 가정하여 임의의 data 3개를 지정하고, 3개의 다른 영역 구성
>
> step2. 3개의 영역을 나눌때, 지정된 3개의 data에서 가장 가까운 data를 계산해 그룹 형성(euclidean distance 사용)
>
> step3. 그룹내에서 데이터의 위치를 계산하여 평균을 내고, 평균을 기준으로 step2를 적용 반복하여 clustering 생성
>
> ![](.\image\yolov2_1.png)

이를 이용해 최적의 anchor box를 찾으면 문제가 발생하게 됩니다.

![](.\image\yolov2_2.png)

파란박스가 GT이고 빨간박스가 anchor box라고 할때, 왼쪽의 사진은 box가 비슷함에도 불구하고 중심점의 차이가 많이나기 때문에 무시될 수 있습니다.

그렇다고 euclidean distance를 기준으로 그룹을 하게 된다면 중간이나 오른쪽 과 같이 이상한 anchor box가 그룹될 확률이 높아집니다.

따라서 논문에서는 IoU를 이용한 distance metric이라는 방식을 제안하였습니다.

![](.\image\yolov2_3.png)

식은 위와 같으며, 이 방법을 통해 GT bbox의 평균을 잘 계산할수 있어 좋은 anchor box를 추출할 수 있게 됩니다.

![](.\image\yolov2_figure2.png)

### Direct location prediction

predict box의 좌표를 학습하여 띄워주게 되는데, 학습 초반의 경우 이 좌표가 학습이 제대로 되어있지 않아 cell을 벗어나 형성되는 문제가 나타나게 됩니다.

![](.\image\yolov2_4.png)

이것을 막기위해 논문에서는 아래와 같은 방법의 box regression식을 사용하였습니다.

![](.\image\yolov2_5.png)

총 5개의 좌표(x, y, w, h, t_o)가 계산하여 나타나게 됩니다.

> t_o는 cell에 대한 상대적인 값

위의 식은 기존에서 사용하던 Faster RCNN에서의 식에 sigmoid 함수를 적용한 것입니다.

sigmoid 함수를 적용하게 되면 x, y의 범위가 0~1사이의 값으로 바뀌기 때문에 cell영역에 제한되게 합니다.

따라서 학습시간도 줄어들고 더 좋은 box를 그려주게 됩니다.

이를 사용하여 약 5%의 성능향상이 있었습니다.(recall? presicion?)

### Fine-Grained Features

작은 feature map에서 작은 크기의 객체의 localization작업은 어렵다고 합니다. 따라서 SSD에서 사용한 것과 비슷하게 feature map size를 키워 작은 객체도 잘 detection할 수 있게 한 방법을 소개합니다.

본 모델의 최종 output feature map은 13\*13의 크기를 갖는데, 이것보다 더 큰 26\*26의 output feature map에서도 detection 작업을 진행합니다.

따라서 최종적으로 SSD에서와 비슷한 모양을 볼 수 있습니다.

![](.\image\yolov2_6.png)

마지막에 125채널을 갖고있는데, 이는 하나의 anchor box에 대해 25가지를 구성하고 있으며, 5개의 box를 적용하였기 때문에 5\*25=125를 갖고있게됩니다.

> (x, y, w, h) 4개
>
> 1개의 confidence 
>
> 20개의 class
>
> 5개의 anchor box

### Multi-Scale Training

anchor box를 사용하기 위해 448크기의 입력을 416으로 변경하여 사용하였습니다.

이렇게 크기를 변경하여서 사용할 수 있는 이유는 모델 자체가 conv와 pooling으로 이루어져 있기 때문에 크기를 자유롭게 변경할 수 있다고 합니다.

따라서 고정된 크기의 입력을 사용하는 대신, 10번의 배치마다 랜덤하게 크기를 변경하고 학습을 진행하였습니다.

> filter의 크기는 변하지 않고, grid cell의 크기가 변하는것

단, 입력이 32배수로 다운샘플링 되어있기 때문에 랜덤한 크기는 32배수로 결정이 되어야 했습니다.

이 방법을 통해 288\*288에 대해서 90FPS와 Fast RCNN과 같은 약 70mAP를 나타내었으며, 544\*544에 대해서는 40FPS와 78.4mAP를 나타내었습니다.

![](.\image\yolov2_table3.png)

![](.\image\yolov2_figure4.png)

## Faster

### Darknet-19

논문에서 새로운 모델을 제안하였습니다.

VGG와 비슷하지만, 3\*3conv filter에 feature map을 압축할 수 있도록 1\*1 filter를 추가하였으며, batch normalization을 사용하였습니다.

따라서 19개의 conv layer와 5개의 max pooling layer를 갖게되었습니다.

> v1과의 차이점: FC layer제거
>
> 속도가 빨라진 이유? - Faster rcnn과 비슷한데 느려져야 하는게 맞지 않나?
>
> FC layer를 제거해 파라미터의 수를 매우 크게 줄일 수 있었음 -> 빠른속도가 되는 원인 1
>
> global average pooling사용 -> 빠른속도가 되는 원인2
>
> v1의 파라미터 수: 7\*7\*1024\*4096+4096\*7\*7\*30
>
> v2의 파라미터 수: 1000

![](.\image\yolov2_table6.png)

## Stronger

9000개의 classification을 위해 class를 9000까지 늘린 방법에 대해 설명하고 있습니다.

### Hierarchical Classification

ImageNet의 dataset과 coco의 detection dataset은 label이 다르기때문에 단순히 합치는 것은 맞지 않다고 합니다.

> ex) Norfolk terrier와 dog

따라서 WordNet구조로 Label을 묶는 작업을 하였습니다.

Root단어에서 연관된 단어를 묶어나가기 때문에 Label data는 1개의 Label을 갖는것이 아닌 유사한 label까지 모두 갖게 되었습니다.

이를 Bayes' theorem을 통해 계산하였습니다.

또한 probability를 계산할 경우 tree를 기준으로 softmax를 적용하였습니다.

![](.\image\yolov2_figure5.png)

### Dataset combination with WordTree

![](.\image\yolov2_figure6.png)

Hierarchical Classification을 ImageNet과 COCO 데이터에 적용한 것입니다. 연관있는 단어에 tree구조로 연결 되어 있는것을 확인할 수 있습니다.

### Joint classification and detection

wordtree를 구성하는 단계에서 detection data와 classification data의 비율이 맞지 않기때문에 두 data의 비율을 4:1로 맞추기 위해 coco data를 oversampling하였습니다.

detection data가 input인 경우 IOU가 0.3이상인 경우에 한하여 backpropagation을 진행하였습니다.

classification data가 input인 경우 classification에 대해 backpropagation을 진행하였는데, 이때 아래의 node까지 error가 전파될 수 있도록 구성하였습니다.