---
layout: post
title: Fast R-CNN
data: 2021-02-15
excerpt: "Fast R-CNN"
tags: [r-cnn, network]
coments: false
mathjax: true
---

# Fast R-CNN

IEEE학술지의 CVPR2015에서 공개된 본 논문의 제목은 그대로 Fast R-CNN입니다. 저자는 마이크로소프트 연구원인 Ross Girshick이며, 2021년 2월 기준 약 1만3천회가 인용되었습니다.

본 논문은 기존의 R-CNN의 단점을 보완하고자 하였습니다. train시에는 VGG16네트워크를 사용하여 기존의 R-CNN보다 9배 더 빨랐으며, test시에는 213배 더 빨랐으며, SPPnet의 경우에 train시 3배, test시 10배 더 빨랐습니다.

또한 PASCAL VOC 2012에서 더 높은 mAP를 달성하였습니다.

## Introduction

object detection은 복잡한 방법을 요구합니다.

1. 수 많은 proposal 후보들이 생성
2. 정확한 localization을 위한 많은 후보 정제 작업

이러한 복잡한 방법때문에 현재사용되는 학습방법은 매우 느립니다.

본 논문에서는 single-stage 알고리즘을 통해 object proposal를 분류하는 것과 spatial location을 정제하는 두 가지 방법을 제안합니다.

위의 방법을 사용하여 PASCAL VOC 2012에서 66%의 mAP를 달성하였습니다.

## R-CNN and SPP net

R-CNN에는 다음과 같은 단점이 존재합니다.

1. Training시 multi-stage pipline(bounding box와 classification이 따로 학습)
2. Training시 시간적, 공간적 비용이 높음
3. Object detection이 느림

R-CNN이 느린 이유는 ConvNet이 forward pass를 수행하는데 연산을 공유하지 않기 때문입니다.

SPPnet의 방법은 전체 입력 이미지에 대해서 convolutional feature map을 계산한 다음 공유된 feature map에서 추출한 feature vector를 통해 object proposal을 분류한 방법으로 R-CNN보다 더 빠른 속도를 나타낼 수 있었습니다.

하지만 SPPnet도 R-CNN처럼 학습과정이 multi-stage로 구성되어 있으며, convolutional layer를 업데이트 할 수 없어 network의 정확도에 제약이 생기게 됩니다.

따라서 본 논문에서는 이러한 R-CNN, SPPnet의 단점을 개선, 속도와 정확도를 향상시키는 새로운 알고리즘을 제안하였습니다. train과 test시에 기존 방법보다 비교적 빠르기 때문에 이러한 방법을 fast R-CNN이라고 명명하였습니다.

fast R-CNN의 장점은 다음과 같이 설명합니다.

1. 기존의 방법보다 높은 detection 성능(mAP)
2. multi-task loss를 이용한 single-stage train
3. 학습이 모든 network layer에 update 가능
4. 디스크 저장소의 불필요

## Fast R-CNN architecture and training

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_fig1.png?raw=true)

Fast R-CNN의 구조는 위의 사진과 같습니다.

먼저 input으로 전체 이미지와 object proposal set을 입력을 받습니다. 그 후 전체 이미지를 conv, max-pooling하여 feature map을 생성합니다.

각 object proposal에 대해서 ROI pooling layer는 feature map으로부터 길이가 고정된 feature vector를 추출합니다.

추출된 각 feature vector는 output layer로 나뉘는 fc layer에 입력됩니다.

최종 출력으로는 softmax 확률 추정치와 bounding box좌표를 출력하게 됩니다.

### The ROI pooling layer

ROI pooling layer는 max-pooling을 사용해 ROI의 feature를 H\*W로 고정된 작은 feature map으로 변환합니다.

> H와 W는 ROI와 독립적인 하이퍼파라미터

각 ROI는 r, c, h, w의 좌표로 표현됩니다. r과c는 좌측 상단을 의미하고, h와w는 높이와 넓이를 의미합니다.

feature map위에 $\frac{h}{H}*\frac{w}{W}$크기 만큼 grid를 만든 후, max-pooling을 하게 되면 고정된 크기인 H\*W크기의 feature size로 바뀌게 됩니다.

따라서 본 논문에서 ROI pooling 순서는 다음과 같습니다.

1. selective search를 적용하여 Conv layer를 통해 추출된 feature map에  ROI를 추출

2. 선별된 ROI를 모두 H\*W(7\*7) size로 만들어 주기 위한 작업

   1. ROI가 h, w크기를 갖고있을때, window size, stride, max_pooling을 사용해 H\*W(7\*7) feature map이 만들어짐

      > SPPnet에서 이러한 방법을 spatial bin이라고 칭함

3. 이렇게 생성된 feature map이 flatten되어 2개의 FC layer를 통과

4. 통과된 feature들은 softmax, bbox regressor에 사용

SPPnet에서 사용한 spatial bin은 1\*1, 2\*2, 4\*4 총 3가지의 spatial bin을 사용하였는데, 논문에서는 7\*7 한 가지만 사용하였는데, 이것은 overfitting 문제를 해결할 수 있게 됩니다.

### Initializing from pre-trained networks

저자는 3개의 pre-trained된 ImageNet network로 실험을 진행하였습니다.

pre-trained network가 Fast R-CNN을 초기화 하면서 세 가지의 변화가 있었습니다.

1. 마지막 max-pooling layer가 ROI pooling layer로 바뀜
2. 마지막 FC layer와 softmax가 softmax와 bounding box regressor로 바뀜
3. 이미지목록과 이미지의 ROI목록, 두 가지의 데이터를 입력받도록 바뀜

### Fine-tuning for detection

back-probagation으로 network의 모든 가중치를 학습하는 것이 Fast R-CNN에서의 중요한 능력입니다.

Fast R-CNN이 학습할 때에는 SGD mini-batches는 계층적으로 sampling되기 때문에 같은 이미지로부터의 ROI는 forward/backward 계산시 연산과 메모리가 공유됩니다. 이것은 연산이 줄어들어 속도가 빨라지는 현상을 보여줍니다.

저자는 같은 이미지에서 ROI가 상호연관 되어있기 때문에 수렴속도가 느려질 수 있는 문제에대해 기존의 R-CNN보다 적은 SGD iteration을 사용하여 좋은 결과를 얻어냈기 때문에 이것은 걱정하지 않아도 된다고 합니다.

### Multi-task loss

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_s1.png?raw=true)

<img src="https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_s4.png?raw=true" style="zoom:130%;" />

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_s2.png?raw=true)

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_s3.png?raw=true)

Fast R-CNN의 loss function을 위와 같이 사용하였습니다.(huber loss, gauss newton algorithm)

이는 classification과 localization loss를 합친 function으로 한 번의 학습으로 둘다 학습시킬 수 있다고 합니다.

> softmax를 통해 얻어진 값 = p
>
> ground-truth class = u
>
> candidate bounding box(특정 클래스의 ROI) = t
>
> ground-truth bounding-box regression target = v
>
> background의 경우 u=0이기 때문에 background ROI에 대해 ground-truth bounding-box 와 두번째 식의 개념이 무시

참고 - https://yeomko.tistory.com/15

### Sampling

R-CNN에서 128개의 미니배치를 사용하고 서로 다른 이미지로부터 128개의 ROI를 취하는 region-wise-sampling방법을 사용하였습니다. 하지만 이 방법을 그대로 Fast R-CNN에 적용하게 되면, 속도가 느려질 가능성이 있습니다.

R-CNN 에서는 224\*224 크기로 wrap하였지만, Fast R-CNN은 원본 이미지를 사용하기 때문입니다. 따라서 ROI가 해당되는 receptive field의 크기가 매우 커져 연산량이 매우 많아지는 결과가 나타납니다. 

본 논문에서는 이 문제를 해결하기 위해 무작위로 ROI를 선택하는 것이 아니라, 작은 수의 학습 영상에서 128개의 ROI를 정하도록 하였습니다. 논문에서는 2장의 학습 영상을 사용하였습니다.

따라서 2개의 이미지에 대해 각각 64개의 ROI가 추출되었으며, IoU > 0.5인 경우 positive로 하였습니다.

## Result

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_table_1t3.png?raw=true)

각기 다른 방법과 데이터셋으로 test한 결과입니다.

모든 방법은 pre-trained VGG16 network로 초기화가 되었습니다.

모든 table의 mAP를 보면 Fast R-CNN의 mAP가 가장 높은 것을 확인할 수 있습니다.

#### Training and test time

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/fast%20rcnn/fast%20rcnn_table_t4.png?raw=true)

S, M, L은 사용한 모델이 다른 것을 나타냅니다.

> S = CaffeNet
>
> M = depth는 S와 같고 넓이는 VGG
>
> L = VGG16

세가지 다른 모델에서 Fast R-CNN의 속도가 빠른 것을 확인할 수 있습니다.

저자는 feature들을 저장하지 않기 때문에 고용량의 디스크가 필요없을 뿐더러 속도의 향상이 있다고 말하고 있습니다. 또한 SVD는 mAP를 조금 떨어트리지만 추가적인 fine-tuning없이 detection 시간을 30%보다 더 줄여 속도를 향상시켰다고 합니다.

### Truncated SVD

Fast R-CNN의 마지막 FC layer에는 Truncated SVD기법이 적용되었습니다.

보통 Conv layer에서 더 많은 시간이 걸리지만 Fast R-CNN의 경우 한번의 CNN과정을 거쳐 약 2000개의 ROI가 추출되는데, 추출된 ROI를 갖고 FC layer에 진행하기 때문에 FC layer에서 더 많은 시간이 소요하게 됩니다.

따라서 굉장히 많은 FC layer에서의 연산이 truncated SVD 기법을 통해 parameter수가 감소하여 test 시간 또한 크게 감소하였습니다.

## Conclusion

ROI pooling을 추가함으로 써 region proposal 연산을 기존 2000\*CNN연산을 1번의 CNN연산으로 줄이게 되었습니다.

또한 feature vector는 기존의 region proposal을 projection 후 연산 한 것이기 때문에 해당 output으로 classfication과 bounding box regression까지 학습이 가능하였습니다.

하지만 Fast R-CNN에서도 사용된 Selective search 알고리즘은 CPU를 통한 연산이라는 한계점이 있습니다.