---
layout: post
title: YOLACT(You Only Look At CoefficienTs)
data: 2021-08-10
excerpt: "YOLOACT"
tags: [yolo, segmentation, detector]
coments: false
mathjax: true
---

# YOLACT(You Only Look At CoefficienTs)

- 제목: YOLACT Real-time Instance Segmentation
- 저자: Daniel Bolya외 4명
- 인용수: 328회
- 학술지:  ICCV 2019



## Introduction

- Image segmentation은 이미지의 모든 픽셀에 라벨을 할당하는 작업으로 Semantic Segmentation과 Instance Segmentation이 존재

  > Semantic Segmentation: 동일한 클래스에 해당하는 픽셀을 모두 같은 색으로 지정
  >
  > Instance Segmentation: 동일한 클래스여도 다른 사물이라면, 다른 색으로 지정
  >
  > ![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/diff_segmentation.png?raw=true)

- 기존의 instance segmentation은 잘 만든 object detection에 병렬적으로 모델을 추가하였으며 segmentation을 one-stage로 구성하기에는 매우 어려움

  > e.g. mask R-CNN

- two-stage의 모델은 RoI align와 같은 것을 사용하여 feature localization에 중점적으로 mask를 생성하여 real-time으로 사용하기 어려움. 즉, RoI영역을 추출하지 않으면서 fully convolution으로 localization을 수행하도록 학습
- 본 논문에서는 Instance Segmentation을 real-time으로 구현하는 것을 목표로 localization을 포기한 YOLACT를 제안
- localization을 포기한 대신 다음 두 개의 task를 병렬적으로 수행
  1. generating a dictionary of non-local *prototype masks over the entire image*
  2. predicting a set of *linear combination coefficients per instance*

- instance마다 예측된 coefficient를 통해 생성된 prototype mask를 linear하게 합친 후, 예측된 bounding box의 크기로 image를 crop함
- 이러한 방식으로 network가 스스로 시각적(visually), 공간적(spatially), 의미적(semantically)으로 비슷한 instance를 다르게 나타내는 prototype instance mask가 localization하는 방법을 학습하게 됨

> prototype mask의 수는 카테고리의 수와 무관하기 때문에 더 많은 카테고리가 존재할 수 있음

- YOLACT는 prototype의 조합으로 분할된 표현을 학습

  > 공간적으로 분리, localization 수행, 윤곽선 검출, 위치 감지를 combination하여 학습

<img src="https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/fig5.png?raw=true" style="zoom:50%;" />

> 1~3번은 소프트하고 잘 안보이는 boundary의 한쪽에 있는 객체를 보여줌
>
> 4번은 객체의 왼쪽 하단을 활성화
>
> 5번은 배경과 객체사이의 경계를 활성화
>
> 6번은 배경이라고 생각하는 부분을 활성화



## YOLACT

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/fig2.png?raw=true)

- RetinaNet을 바탕으로 feature localization이 없는 mask branch를 추가한 형태의 network

- feature localization이 없는것 대신 위에서 설명한 두 가지의 작업을 병렬로 처리되는 것을 확인

  > protonet, prediction head


## Protonet branch
### Prototype Generation

- Protonet에서 Prototype을 생성

<img src="https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/fig3.png?raw=true" style="zoom:75%;" />

- feature map에서 p3를 input으로 사용
- output으로 설정한 k개의 prototype mask가 생성. 이때, background를 확실히 구분하기 위하여 ReLU를 사용

## Prediction Head branch

### Mask Coefficients

- anchor를 사용하는 일반적인 object detection의 경우 anchor당 4+c개의 coefficient를 생성하는데, 본 논문에서는 거기에 더하여 k개의 prototype이 병렬적으로 추가된 형태이기 때문에 anchor당 4+c+k개의 coefficient가 생성
- mask prototype중 하나가 매우 큰 오류를 나타내면 마스크가 사라지는 등의 문제가 발생하기 때문에 tanh activation을 적용하여 안정적인 output을 출력

- Protonet에서는 특별한 loss를 설정하지 않음

  > 생성되는 prototype mask에서 activate되는 정도를 제한하지 않기 위해

## Mask Assembly

- 최종적으로 prototype mask에 mask coefficient를 linear combination하여 사용

$$
M=\sigma(PC^T)
$$

> P: prototype mask(h \* w \* k)
>
> C: NMS를 통과한 mask coefficients(n \* k)

- 위의 공식과 같이 P와 C의 두 행렬을 matrix multiplication후 sigmoid를 적용하여 최종적인 mask를 출력

### Losses

- loss는 classification loss, box regression loss, mask loss가 존재
- classification과 box loss는 SSD에서 사용한 방법과 똑같이 계산
- mask loss의 경우 GT와의 pixel-wise binary cross entropy를 사용하여 계산
- 각 loss에 1, 1.5, 6.125의 weight를 주어 사용

$$
L=L_{cls}+L_{box}+L_{mask}\\
L_{cls}: Softmax Cross Entropy\\
L_{box}: smooth_{L1}\\
L_{mask}: Binary Cross Entropy
$$

### Cropping Masks

- 최종적으로 mask를 crop해서 사용

- evaluation일때, 예측된 bounding box를 사용하여 crop

- training일때는 GT의 bounding box를 사용하여 crop

  > 더 작은 객체를 잘 보존하기 위해

## Fast NMS

- 본 논문에서는 기존의 NMS보다 성능이 조금 떨어지나 속도가 향상된 Fast NMS를 제시

- 기존의 NMS와의 차이점은 비교기준으로 설정한 box를 다른 box와 비교하였을때, 기준이되는 box가 다른 box보다 score가 낮다면 기준되는 box를 지우는 것
  다시말해 자기보다 score와 iou threshold가 큰것이 발견되면, 자기자신을 더 낮은 score로 판단하여 자신을 지우는 것
- 자세한 Fast NMS의 동작
  1. class에 대해 score로 box 정렬
  2. box들끼리 서로 iou를 계산하여 iou matrix 생성
  3. 생성된 iou matrix에서 우측 상단 값을 제외한 나머지 값은 삭제
  4. iou matrix에서 각 column에서 가장 큰 값을 뽑아 K matrix를 생성
  5. K matrix에서 iou threshold보다 작은 값은 그대로 두고, 큰 값은 삭제

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/fast%20NMS.png?raw=true)

- 또 다른 차이점은 기존 NMS에서는 삭제된 box는 다른 box와 비교를 진행하지 않지만, Fast NMS는 삭제되더라도 다른 box와 비교를 진행
- 기존 NMS는 순차적으로 이뤄지지만, Fast NMS는 행렬연산으로 병렬로 이루어지기 때문에 GPU로 수행 가능
- Fast NMS를 사용하여 최대 12ms의 속도 향상

## Results

- COCO dataset과 Pascal 2012 SBD를 사용하여 실험을 진행하였지만, 본 논문에서는 COCO에 대한 결과만 작성

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/table1.png?raw=true)

> 뒤에 붙은 숫자는 input resolution을 의미
> e.g. YOLACT-550은 550*550의 input resolution을 사용

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/table2.png?raw=true)

![](https://github.com/jh79783/jh79783.github.io/blob/main/assets/img/yolact/fig7.png?raw=true)

