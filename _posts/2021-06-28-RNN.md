---
layout: post
title: RNN(Recurrent Neural Networks)
data: 2021-02-12
excerpt: "RNN"
tags: [RNN]
coments: false
mathjax: true
---

- RNN은 Sequence 모델 즉, 입력과 출력을 Sequence 단위로 처리하는 것

- Sequence 길이에 상관없이 유연하게 구조를 만들 수 있음

  > Sequence: 순서가 있는 data
  >
  > ex) Text에는 문맥이라는 순서, 시계에는 시간이라는 순서

![](.\image\RNN_loop.png)

- x: 입력 벡터

- A: 메모리 셀 or RNN 셀 

  > 은닉층에서 이전 값을 기억하는 메모리 역할

- y: 출력 벡터

![](.\image\RNN_total_network.png)

- 셀이 출력층 방향 또는 다음 시점 자신에게 보내는 값을 hidden state라고 함

- 즉, t시점의 셀은 t-1시점의 셀의 값을 입력으로 받아 t시점의 hidden state 계산을 위한 값으로 사용

- 은닉층은 은닉 상태값을 계산하기 위해 두 개의 가중치를 갖고있음

  > 입력층에서 은닉층으로 가는 가중치: $W_x$
  >
  > 이전 상태의 t-1에서 현재상태 t로 가는 가중치: $W_h$
  >
  > 은닉층에서 출력층으로 가는 가중치: $W_y$
  >
  > 같은 층에 있는 가중치는 share 되어 있음

- 은닉층의 상태값은 다음과 같이 표현됨

$$
은닉층: h_t=tanh(W_xx_t+W_hh_{t-1}+b)
$$

> 은닉층에서의 활성화 함수를 꼭 tanh만 써야하나??
>
> ReLU를 시도한 적이 있음 결과는??
>
> RNN은 내부를 계속해서 순환하는 구조 -> ReLU는 1보다 크게 되면 값이 발산할 수 있음
>
> 발산하지 않는 데이터는 어떻게 되나??

- tanh를 쓰는 이유
  - RNN에서의 Vanishing gradient문제를 예방하기 위해
  - tanh를 사용하면 gradient가 최대한 오랫동안 유지하도록 도움을 줌

![](.\image\RNN_sigmoid vs tanh.png)

> sigmoid에 비해 tanh의 미분 최대 값이 상대적으로 크다 -> gradient를 유지하기 쉽다

## RNN의 문제

### The Problem of Long-Term Dependencies(장기의존성 문제)

- RNN의 매력은 현재 task에 과거의 정보를 연결하는 것

  > 이전의 frame을 통해 현재의 frame을 이해하는 것

- 연관되어있는 정보와 그에관한 정보를 사용하는 곳의 거리가 가까우면 학습이 잘됨

- 그러나, 거리가 멀게되면 학습능력이 저하되는 현상이 나타남

  > 역전파시 vanishing gradient problem 발생

- 이를 해결하기 위해 Long Short Term Memory networks(LSTM) 등장

## LSTM

![](.\image\RNN_LSTM.png)

- 기본적인 RNN은 tanh layer하나만 있는 체인형식 구조
- LSTM도 체인형식 구조이지만 4개의 interacting layer로 구성되어있음

### The Core Idea Behind LSTMs

- LSTM의 핵심은 cell state
- 모듈 위쪽에 있는 선이 cell state

![](.\image\RNN_cell_state.png)

- 이 선은 컨베이어와 비슷하게 체인 구조를 처음부터 끝가지 지나가도록 구성
- 지나가면서 약간의 선형 상호작용(\*, +)를 하기 때문에 정보는 거의 변하지 않게 됨
- 즉, LSTM은 이 cell state에 gate라는 구조를 통해 정보를 통제할 수 있음
- gate는 sigmoid와 pointwise 곱연산으로 이루어져 있음

![](.\image\RNN_gate.png)

- sigmoid layer는 0~1의 값을 전달함

  > 각 요소들을 얼마나 많이 통과시킬지 결정하는 것
  >
  > 0: 아무것도 통과시키지 않음
  >
  > 1: 모든 것을 통과시킴

- LSTM은 cell state를 보호하고 컨트롤 하기위해 3개의 gate를 갖고 있음

### Step-by-Step LSTM Walk Through

- 1 단계: cell gate로 어떤 정보를 흘려 보낼 것인가?

  - sigmoid를 통해 결정

    > forget gate layer라고 부른다

  - $H_{t-1}, x_t$를 sigmoid에 통과시켜 0~1의 값을 cell state에 보내게 된다

    > 0: 모두 지움
    >
    > 1: 모두 저장함

![](.\image\RNN_LSTM-forget_gate_layer.png)

- 2 단계: cell state에 저장할 새로운 정보를 결정
  - 1. input gate layer가 어떤 값을 update할지 결정
    2. tanh layer가 $\tilde{C}_t$라는 cell state에 더해질 수도 있는 새로운 후보 벡터를 생성
    3. 시그모이드를 통과한 값과 새로운 후보 벡터를 결합하고 cell state에 update

![](.\image\RNN_LSTM-input_gate.png)

- 3 단계: output 결정

  - output은 cell state에 기반하여 출력됨

  - 1. sigmoid를 통해 cell state에서의 output 결정

       > cell state는 온전한 cell state가 아닌 filter된 값
       >
       > 즉, 이전 상태에서의 output

    2. cell state를 tanh에 넣어 sigmoid의 결과와 곱해줌

![](.\image\RNN_LSTM-output.png)

## Variantes on Long Short Term Memory

- LSTM의 변형된 형태를 소개
  - 기존의 LSTM에 peephole connections을 추가한 방식
  - 기존의 LSTM에 coupled forget/input gate를 추가한 방식
  - Gated Recurrent Unit(GRU)인 대부분이 바뀐 방식
  - GRU는 forget gate와 input gate를 합쳐 update gate로 바꿈
  - 또한 cell state와 hidden state를 합침
  - 이것이 기존의 LSTM보다 간단하고 더 많이 이용되는 추세

![](.\image\RNN_LSTM-peephole.png)

![](.\image\RNN_LSTM-coupled.png)

![](.\image\RNN_GRU.png)